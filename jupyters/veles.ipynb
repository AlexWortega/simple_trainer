{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa5b4be-057c-4f2a-9bbf-bf77587f899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.36.1 peft datasets trl sentencepiece dataset wandb flash_attn accelerate --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174e1c34-b581-4172-8fdb-656eeba4fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da220961-0b4d-487f-9c8c-9068872ad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33642bf8-647e-4e43-9251-2ecceb614cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccad9296341d49aeade85f24eb6cd7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"AlexWortega/v4\"\n",
    "new_model = \"mistral-vikhr-7b\"\n",
    "\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(  \n",
    "#     load_in_8bit= True,\n",
    "#     bnb_8bit_quant_type= \"nf4\",\n",
    "#     bnb_8bit_compute_dtype= torch.float16,\n",
    "#     bnb_8bit_use_double_quant= False,\n",
    "# )\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        # quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.config.use_cache = False \n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d129a56-b032-4c6b-aba5-3dd9087b2277",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbd35e2-fc87-4737-b594-001f8267ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token=False, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c632026-6f0d-4a3f-bcc7-a8328fd60c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e57e8a-9451-4479-b4e7-3ff7a9141b0e",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8203c9-a958-44e7-a7e5-16f13f79cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"Vikhrmodels/Veles-2.5\", split=\"train\")\n",
    "import json\n",
    "\n",
    "def dict_to_chatml(data):\n",
    "    chatml = []\n",
    "    for item in data['conversations']:\n",
    "        if item['from'] == 'system':\n",
    "            chatml.append({\"role\": \"system\", \"content\": item['value']})\n",
    "        elif item['from'] == 'human':\n",
    "            chatml.append({\"role\": \"user\", \"content\": item['value']})\n",
    "        elif item['from'] == 'gpt':\n",
    "            chatml.append({\"role\": \"assistant\", \"content\": item['value']})\n",
    "    return {\"messages\":chatml}\n",
    "dataset_ft=dataset.map(dict_to_chatml)\n",
    "rmv=dataset_ft.column_names\n",
    "rmv.remove(\"messages\")\n",
    "dataset_train=dataset_ft.remove_columns(rmv)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Vikhrmodels/Vikhr-7b-0.2\", use_fast=False, add_eos_token=True)\n",
    "additional_special_tokens = [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': additional_special_tokens})\n",
    "tokenizer.model_max_length=1512\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "tokenizer.add_bos_token=False\n",
    "tokenizer.add_eos_token=False\n",
    "tokenizer.chat_template = \"<s>{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}</s>\"\n",
    "def try_apply_chat_template(messages, tokenizer):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying chat template to messages. Error\")\n",
    "        return None\n",
    "dataset_train_ = dataset_train.map(lambda x: {\n",
    "    \"formatted_chat\": try_apply_chat_template(x['messages'], tokenizer)\n",
    "})\n",
    "\n",
    "\n",
    "dataset_train_ = dataset_train_.filter(lambda x: x['formatted_chat'] is not None)\n",
    "dataset_train=dataset_train_.remove_columns('formatted_chat')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "os.environ[\"WANDB_PROJECT\"] = \"vikhrlora_fp16_v4\"  # name your W&B project\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint_vikhrlora_fp16_v4\"\n",
    "# test_data = load_dataset(\"facebook/flores\", \"eng_Latn-deu_Latn\", streaming=False, \n",
    "#                           split=\"devtest\")\n",
    "\n",
    "# def preprocess_func(row):\n",
    "#   return {'text': \"Translate from English to German: <s>[INST] \" + row['sentence_eng_Latn'] + \" [INST] \" + row['sentence_deu_Latn'] + \" </s>\"}\n",
    "\n",
    "\n",
    "# valid_dataset = valid_data.map(preprocess_func)\n",
    "# test_dataset = test_data.map(preprocess_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8708de3d-f6b9-485c-955e-8cc533570aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexwortega\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login() #e461a6a3bca9f7cec3390a40dc10cdf576ce3252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321944d-4206-4feb-9a3b-366bcab17477",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05ed950-5f72-4332-92c1-5bc0e6b01b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81575a18-3b40-47e2-9bba-16365fdce9f3",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae7e8b0-8a9e-4de5-9ace-8030b802de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "max_seq_length = 1512 # max sequence length for model and packing of the dataset\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"vikhr-7b-fp16-lora-chatml-mistral\", # directory to save and repository id\n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=False,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",  \n",
    "    # max_seq_length=max_seq_length,\n",
    "    fp16=True,# save checkpoint every epoch\n",
    "    # bf16=True,                              # use bfloat16 precision\n",
    "    # tf32=True,\n",
    "    # use tf32 precision\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # use constant learning rate scheduler\n",
    "    push_to_hub=False,                       # push model to hub\n",
    "    report_to=\"wandb\",                # report metrics to tensorboard\n",
    ")\n",
    "import datasets\n",
    "datasets.builder.has_sufficient_disk_space = lambda needed_bytes, directory='.': True\n",
    "tokenizer.model_max_length=1512\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "from trl import SFTTrainer\n",
    "import transformers\n",
    "tokenizer.model_max_length=1512\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_seq_length = 1512 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset_train,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False, # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c0ff9-8881-4ef1-83e1-30ecf9342e33",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305501e-f6f8-48cd-a08b-c2871247df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f96e5c35-3fa9-42b0-b5a4-b3355a200aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "690bbbfc-4626-4905-a018-de741d061886",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"output-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eda0971-3f62-4ab2-ba9e-91012d2f55e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae865e2b4339438f9cc7a7b2d0ce550c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/AlexWortega/vikhr-7b-v4-chatml-veles/commit/c6f057756df147074e214aa25ee859e1606ddfc0', commit_message='Upload model', commit_description='', oid='c6f057756df147074e214aa25ee859e1606ddfc0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"vikhr-7b-v4-chatml-veles\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e57752-fc10-4975-9711-343147f5a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "moddl = AutoModel.from_pretrained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
